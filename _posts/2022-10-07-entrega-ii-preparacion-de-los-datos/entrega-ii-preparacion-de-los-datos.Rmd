---
title: "Entrega II - Impacto de los cambios de los componentes ópticos oculares en la refracción de los jóvenes"
description: |
   En este post se presenta la primera fase del análisis de datos - preparación de datos - en la que se analizan las características de las variables y se procesa el tratamiento de los datos. Además, se realiza un análisis gráfico de los datos.
author:
  - name: Ines Pereira
    url: https://example.com/norajones
    affiliation: Spacely Sprockets
    affiliation_url: https://example.com/spacelysprokets
date: 2022-10-07
output:
  distill::distill_article:
    self_contained: false
---

## Base de datos
```{r}
data <- read.csv("base_datos.csv", header = T, sep = ",", dec = ".")
data <- data.frame(data)
```

## Características de las variables
```{r}
str(data)
summary(data)
##numerical variables with missing values: J0, J45, cil, eje
data$sexo = as.factor(data$sexo)
data$ametropia = as.factor(data$ametropia)
data$grupo_etario = as.factor(data$grupo_etario)
data$orient_astigm = as.factor(data$orient_astigm)
str(data)
summary(data)
```

Después de analizar las características de las distintas variables, se comprobó que variables que deberían de estar como factores, como la variable binaria sexo y las variables con niveles (ametropia, grupo_etario, orient_astigm) estaban como variables enteras (integer). Por lo tanto, se realizaron los cambios necesarios en el tipo de variables transformándose en factores

## Outliers & Boxplots
```{r}
#see outliers of numerical variables

##variable edad
boxplot(data$edad, col="lightblue", main="Edad Boxplot",
        ylab="Edad")
boxplot.stats(data$edad)

  ##12 outliers

##Variable M
boxplot(data$M, col="lightgreen", main="Graduación Componente Esférica Boxplot",
        ylab="M")
boxplot.stats(data$M)

  ##144 outliers

##Variable J0
boxplot(data$J0, col="lightpink", main="Componente Vectorial Horizontal Boxplot",
        ylab="J0")
boxplot.stats(data$J0)

  ## 76 outliers

##Variable J45
boxplot(data$J45, col="violet", main="Componente Vectorial Oblicuo Boxplot",
        ylab="J45")
boxplot.stats(data$J45)

  ##66 outliers

##Variable esf
boxplot(data$esf, col="orange", main="Esfera Boxplot",
        ylab="Esfera")
boxplot.stats(data$esf)

  ##161 outliers

##Variable cil
boxplot(data$cil, col="lightyellow", main="Cilindro Boxplot",
        ylab="Cilindro")
boxplot.stats(data$cil)

  ##88 outliers 

##Variable eje
boxplot(data$eje, col="#3391FF", main="Eje Boxplot",
        ylab="Eje")
boxplot.stats(data$eje)

  ##sin outliers

##Variable CA
boxplot(data$CA, col="#CBF4A9", main="Longitud Axial del Ojo Boxplot",
        ylab="CA")
boxplot.stats(data$CA)

  ##25 outliers

##Variable RC
boxplot(data$RC, col="#B7F6DA", main="Radio de Curvatura Boxplot",
        ylab="RC")
boxplot.stats(data$RC)

  ##15 outliers

##Variable CA_RC
boxplot(data$CA_RC, col="#F6B7B7", main="Predictor Defecto Refractivo Boxplot",
        ylab="CA_RC")
boxplot.stats(data$CA_RC)

  ##58 outliers
```

Al analizar los boxplots y la presencia de outliers, se comprobó que 9 variables contenían outliers, y la variable en la que la presencia de outliers era más considerable, era sólo el 10%. Después de la análisis, se decidió no eliminar ninguno de los outliers. Para estos datos analizados, se sabe que es probable que existan todos estos valores, por lo que los outliers no reflejan valores imposibles. Además, según la medición realizada, también se descartan los errores de medición. Así, los valores extremos presentes en estos datos, existen porque reflejan la graduación de una persona que se supone está relacionada con la longitud del ojo de una persona, por ejemplo, una mayor longitud de un ojo, dará una mayor graduación. Como las personas son diferentes y existen distintas longitudes de ojos, es importante analizar incluso las más extremas para conocer la relación entre estas variables. Por lo tanto, no se eliminaron los outliers.

## Missing Values
```{r}
#Nearest Neighbor - Treat missing values

##apply o nearest neighbor (kk), with k=5 because it's a default value
##numerical variables with missing values: J0, J45, cil, eje
library(VIM)
dataKNN<-kNN(data, variable=c("J0","J45","cil","eje"), k=5)
summary(dataKNN)
dataKNN<-subset(dataKNN, select=id:CA_RC)
summary(dataKNN)
```

Después de constatar la existencia de missing values en los datos, se utilizó un método para sustituir estos valores ausentes. Para eso, se utilizó el método de estimación del Nearest-Neighbour (vecino más cercano), que identifica los k vecinos más cercanos de la observación que falta y predice el valor de la variable en esa observación, basándose en el valor encontrado en la mayoría de los vecinos. De este modo, mediante la búsqueda de los 5 vecinos más cercanos, fue posible obtener una estimación adecuada de los valores ausentes para las distintas variables.

## Análisis de componentes principales - PCA

El análisis de componentes principales, PCA, es un método de reducción de la dimensionalidad que se utiliza para reducir la dimensionalidad de grandes conjuntos de datos. Este método se basa en la transformación de un gran conjunto de variables en uno más pequeño, que siga conteniendo la mayor parte de la información del conjunto grande. El objetivo principal de este método es aumentar la simplicidad, pues con un conjunto de datos más pequeño es más fácil y rápido de explorar y visualizar. El primer paso de esta técnica es establecer las correlaciones entre las diferentes variables para entender qué variables están más correlacionadas y, así, poder identificar los componentes principales. A continuación, se presenta la matriz de correlación entre las variables numéricas. 

```{r}
##crear dataframe solo con variables numericas
data_num <- dataKNN[,c(-1:-5,-7,-14)]

##correlaciones entre las variables
cor(data_num) 
library(corrplot)
matriz.cor=cor(data_num,)
corrplot(matriz.cor)
```

Analizando los valores obtenidos en las correlaciones entre las variables y viendo la matriz de correlaciones, se observa que hay algunas variables con correlaciones altas (entre 0,6 y 0,8) que serían buenos indicadores para aplicar la técnica de PCA. Sin embargo, recordando el objetivo principal de esta técnica (reducir el número de variables), para este proyecto, dado que sólo hay 17 variables, y sólo 10 numéricas (que podrían reducirse), se consideró no proceder al análisis de componentes principales porque no es necesario reducir las variables. Por lo tanto, el estudio continuará con todas las variables iniciales.

## Histograma
```{r}
#variable dependiente: M
library(ggplot2)
  ggplot(dataKNN, aes(x = M)) +
  geom_histogram(binwidth = 0.05) +
  labs(x = "Graduación de las personas en estudio determinada en su componente esférica ", y = "Frecuencia",
       title = "Distribución de la variable dependiente")
```
El histograma muestra que la variable dependiente estudiada sigue una distribución aproximadamente normal, con la mayoría de las observaciones concentradas entre +1 y -1 dioptrías. 

## Correlaciones & Scatterplots
```{r}

ggplot(dataKNN, aes(x=CA, y=M)) +
  geom_point(color="lightblue", size=2, shape=23) + 
  labs(x = "Longitud Axial del Ojo", y = "Graduación de las personas",
       title = "Relacion entre la longitud axial del ojo y la graduación de las personas")

ggplot(dataKNN, aes(x=RC, y=M)) +
  geom_point(color="lightpink", size=2, shape=23) + 
  labs(x = "Radio de Curvatura del Ojo", y = "Graduación de las personas",
       title = "Relacion entre la longitud axial del ojo y la graduación de las personas")

ggplot(dataKNN, aes(x=RC, y=CA)) +
  geom_point(color="lightgreen", size=2, shape=23) + 
  labs(x = "Radio de Curvatura del Ojo", y = "Longitud Axial del Ojo",
       title = "Relacion entre la longitud axial y el radio de curavatura del ojo")

ggplot(dataKNN, aes(x=edad, y=CA)) +
  geom_point(color="violet", size=2, shape=23) + 
  labs(x = "Edad de las personas", y = "Longitud Axial del Ojo",
       title = "Relacion entre la longitud axial del ojo y la edad de las personas")
```

Al analizar los gráficos de correlación, se observa que las variables independientes, CA (longitud axial del ojo) y RC (radio de curvatura del ojo) tienen una relación similar con la variable independiente, aunque parcialmente simétrica entre sí. Esto se puede comprobar además en el gráfico de comparación entre estas dos variables independientes, que muestra una relación con tendencia lineal.
Además, al analizar cómo afecta la edad a la longitud de los ojos de las personas, se observa una tendencia al aumento de la longitud con la edad, hasta una determinada edad, y una posterior estabilización de la longitud de los ojos. Esta observación demuestra una posible relación del crecimiento del ojo con el crecimiento de las personas, que se estabiliza después de la pubertad. 
Así, el análisis de estos gráficos nos permitió inferir posibles relaciones entre las distintas variables, que posteriormente serán probadas y analizadas mediante modelos.